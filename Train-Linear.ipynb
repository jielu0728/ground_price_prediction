{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:01:21.068954Z",
     "start_time": "2019-07-18T16:01:20.678159Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.sparse import hstack\n",
    "from collections import defaultdict, Counter\n",
    "from ds_tools.ds_tools import CategoricalTransformer\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:01:21.659616Z",
     "start_time": "2019-07-18T16:01:21.319899Z"
    }
   },
   "outputs": [],
   "source": [
    "df_description = pd.read_csv('./data/data_definition.txt', sep='\\t')\n",
    "\n",
    "df_train_genba = pd.read_csv('./data/train_genba.tsv', sep='\\t')\n",
    "df_train_goto = pd.read_csv('./data/train_goto.tsv', sep='\\t')\n",
    "\n",
    "df_train = df_train_goto.merge(df_train_genba, on='pj_no', how='left')\n",
    "df_train.drop('id', axis=1, inplace=True)\n",
    "\n",
    "df_test_genba = pd.read_csv('./data/test_genba.tsv', sep='\\t')\n",
    "df_test_goto = pd.read_csv('./data/test_goto.tsv', sep='\\t')\n",
    "\n",
    "df_test = df_test_goto.merge(df_test_genba, on='pj_no', how='left')\n",
    "test_surface = df_test['tc_mseki']\n",
    "df_test.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:01:22.680575Z",
     "start_time": "2019-07-18T16:01:22.588119Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_city_name(name):\n",
    "    if '市' not in name and '郡' not in name:\n",
    "        name = '市' + name\n",
    "    return name\n",
    "\n",
    "def split_address(df):\n",
    "    df['jukyo'] = df['jukyo'].str.slice(start=3).str.replace(r'[ヶｹ]', 'ケ')\n",
    "    df['jukyo'] = df['jukyo'].apply(fill_city_name)\n",
    "    city_split = df['jukyo'].str.split(r'[市郡]', n=1, expand=True)\n",
    "    df['city'] = city_split[0]\n",
    "    street_split = city_split[1].str.split(r'[町区]', n=1, expand=True)\n",
    "    df['street'] = street_split[0]\n",
    "    df['address_detail'] = street_split[1].str.strip().replace('', None)\n",
    "    return df\n",
    "\n",
    "df_train = split_address(df_train)\n",
    "df_test = split_address(df_test)\n",
    "\n",
    "df_train.drop(['kaoku_um', 'toshikuiki2', 'shu_sogi'], axis=1, inplace=True)\n",
    "df_test.drop(['kaoku_um', 'toshikuiki2', 'shu_sogi'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:01:24.501867Z",
     "start_time": "2019-07-18T16:01:23.517072Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine(row, combine_list, tup):\n",
    "    l = set()\n",
    "    for col in tup:\n",
    "        if pd.notnull(row[col]):\n",
    "            l.add(row[col])\n",
    "    combine_list.append(','.join(l))\n",
    "\n",
    "\n",
    "combine_cols = [('yoto', 100), ('road_hk', 100)]\n",
    "for i, tup in enumerate([['yoto1', 'yoto2'], ['road1_hk', 'road2_hk', 'road3_hk', 'road4_hk']]):\n",
    "    combine_train = []\n",
    "    combine_test = []\n",
    "    \n",
    "    combine_col_name = combine_cols[i][0]\n",
    "    _ = df_train.apply(lambda row: combine(row, combine_train, tup), axis=1)\n",
    "    _ = df_test.apply(lambda row: combine(row, combine_test, tup), axis=1)\n",
    "\n",
    "    count_vectorizer = CountVectorizer(min_df=combine_cols[i][1])\n",
    "    combine_train_matrix = count_vectorizer.fit_transform(combine_train).todense()\n",
    "    combine_test_matrix = count_vectorizer.transform(combine_test).todense()\n",
    "    for i in range(combine_train_matrix.shape[1]):\n",
    "        df_train['%s_%d' % (combine_col_name, i)] = combine_train_matrix[:, i]\n",
    "        df_test['%s_%d' % (combine_col_name, i)] = combine_test_matrix[:, i]\n",
    "    for col in tup:\n",
    "        df_train.drop(col, axis=1, inplace=True)\n",
    "        df_test.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:01:25.691047Z",
     "start_time": "2019-07-18T16:01:25.269981Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splitter = KFold(n_splits=5, shuffle=True, random_state=28)\n",
    "price_stats = []\n",
    "for train_idx, valid_idx in splitter.split(df_train):\n",
    "    price_stats_by_city = defaultdict(dict)\n",
    "    for city, group in df_train.iloc[train_idx].groupby('city'):\n",
    "        price_list = group['keiyaku_pr']/group['tc_mseki']\n",
    "        price_stats_by_city[city]['price_by_city_mean'] = price_list.mean()\n",
    "        price_stats_by_city[city]['price_by_city_median'] = price_list.median()\n",
    "        price_stats_by_city[city]['price_by_city_min'] = price_list.min()\n",
    "        price_stats_by_city[city]['price_by_city_max'] = price_list.max()\n",
    "        price_stats_by_city[city]['price_by_city_std'] = price_list.std()\n",
    "        price_stats_by_city[city]['price_by_city_count'] = len(price_list)\n",
    "    for i, city in enumerate(df_train.iloc[valid_idx]['city']):\n",
    "        price_stats.append((valid_idx[i], price_stats_by_city[city]))\n",
    "\n",
    "price_stats_test = [] \n",
    "price_stats_by_city = defaultdict(dict)\n",
    "for city, group in df_train.groupby('city'):\n",
    "    price_list = group['keiyaku_pr']/group['tc_mseki']\n",
    "    price_stats_by_city[city]['price_by_city_mean'] = price_list.mean()\n",
    "    price_stats_by_city[city]['price_by_city_median'] = price_list.median()\n",
    "    price_stats_by_city[city]['price_by_city_min'] = price_list.min()\n",
    "    price_stats_by_city[city]['price_by_city_max'] = price_list.max()\n",
    "    price_stats_by_city[city]['price_by_city_std'] = price_list.std()\n",
    "    price_stats_by_city[city]['price_by_city_count'] = len(price_list)\n",
    "for city in df_test['city']:\n",
    "    price_stats_test.append(price_stats_by_city[city])\n",
    "    \n",
    "df_price_stats = pd.DataFrame([x[1] for x in sorted(price_stats, key=lambda x: x[0])])\n",
    "df_price_stats_test = pd.DataFrame(price_stats_test)\n",
    "\n",
    "df_train = pd.concat([df_train, df_price_stats], axis=1)\n",
    "df_test = pd.concat([df_test, df_price_stats_test], axis=1)\n",
    "\n",
    "for col in ['mseki_rd_hb', 'road3_fi', 'rosenka_hb', 'kempei2', 'road2_mg', 'kaoku_hb', 'bus_hon']:\n",
    "    df_train[col].replace(0.0, np.nan, inplace=True)\n",
    "    df_test[col].replace(0.0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:01:26.290385Z",
     "start_time": "2019-07-18T16:01:26.281814Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['station_name_prefix'] = df_train['rosen_nm1'].str.slice(stop=2)\n",
    "df_test['station_name_prefix'] = df_test['rosen_nm1'].str.slice(stop=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:01:29.196613Z",
     "start_time": "2019-07-18T16:01:28.388930Z"
    }
   },
   "outputs": [],
   "source": [
    "continue_features = list(df_description[(df_description['データ型'] == '数値') & (df_description['項目名'] != 'pj_no')]['項目名'])\n",
    "continue_features += ['price_by_city_mean', 'price_by_city_median', 'price_by_city_min', 'price_by_city_max', \n",
    "                      'price_by_city_std', 'price_by_city_count']\n",
    "objective = 'keiyaku_pr'\n",
    "categorical_features = list(df_train)\n",
    "\n",
    "for f in continue_features+[objective]:\n",
    "    if f in categorical_features:\n",
    "        categorical_features.remove(f)\n",
    "        \n",
    "for col in categorical_features:\n",
    "    if col not in ['pj_no']:\n",
    "        ct = CategoricalTransformer(min_freq=3)\n",
    "        df_train[col] = ct.fit_transform(df_train[col])\n",
    "        df_test[col] = ct.transform(df_test[col])\n",
    "        \n",
    "for col in continue_features:\n",
    "    if col != 'keiyaku_pr':\n",
    "        scaler = MinMaxScaler()\n",
    "        df_train[col] = scaler.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "        df_test[col] = scaler.transform(df_test[col].values.reshape(-1, 1))\n",
    "    \n",
    "df_test['keiyaku_pr'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:01:33.657147Z",
     "start_time": "2019-07-18T16:01:29.198323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cv mean l2 0.01407395\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "continue_features.remove('keiyaku_pr')\n",
    "splitter = KFold(n_splits=5, shuffle=True, random_state=28)\n",
    "prediction_list = []\n",
    "best_scores = []\n",
    "df_train.fillna(0, inplace=True)\n",
    "df_test.fillna(0, inplace=True)\n",
    "for train_idx, valid_idx in splitter.split(df_train):\n",
    "    train, valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    test = df_test[df_train.drop(objective, axis=1).columns]\n",
    "    encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "    train_cat_one_hot = encoder.fit_transform(train[categorical_features])\n",
    "    valid_cat_one_hot = encoder.transform(valid[categorical_features])\n",
    "    test_cat_one_hot = encoder.transform(test[categorical_features])\n",
    "    X_train, y_train = hstack([train[continue_features].values, train_cat_one_hot]), np.log(train['keiyaku_pr']+1)\n",
    "    X_valid, y_valid = hstack([valid[continue_features].values, valid_cat_one_hot]), np.log(valid['keiyaku_pr']+1)\n",
    "    regressor = linear_model.Ridge(alpha=2, random_state=28)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    pred_val = regressor.predict(X_valid)\n",
    "    prediction_list.append(regressor.predict(hstack([test[continue_features].values, test_cat_one_hot])))\n",
    "    best_scores.append(mean_squared_error(y_valid, pred_val))\n",
    "    \n",
    "print(\"5-fold cv mean l2 %.8f\" % np.mean(best_scores))\n",
    "\n",
    "df_submission = pd.read_csv('./data/sample_submit.tsv', sep='\\t', names=['id', 'pred'])\n",
    "\n",
    "df_submission['pred'] = np.exp(np.mean(prediction_list, axis=0))-1\n",
    "df_submission.to_csv('submission_ridge.tsv', sep='\\t', header=None, index=False)\n",
    "\n",
    "# 0.01407395"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuberRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:02:10.595561Z",
     "start_time": "2019-07-18T16:01:45.617007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cv mean l2 0.01362982\n"
     ]
    }
   ],
   "source": [
    "splitter = KFold(n_splits=5, shuffle=True, random_state=28)\n",
    "prediction_list = []\n",
    "best_scores = []\n",
    "for train_idx, valid_idx in splitter.split(df_train):\n",
    "    train, valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    test = df_test[df_train.drop(objective, axis=1).columns]\n",
    "    encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "    train_cat_one_hot = encoder.fit_transform(train[categorical_features])\n",
    "    valid_cat_one_hot = encoder.transform(valid[categorical_features])\n",
    "    test_cat_one_hot = encoder.transform(test[categorical_features])\n",
    "    X_train, y_train = hstack([train[continue_features].values, train_cat_one_hot]), np.log(train['keiyaku_pr']+1)\n",
    "    X_valid, y_valid = hstack([valid[continue_features].values, valid_cat_one_hot]), np.log(valid['keiyaku_pr']+1)\n",
    "    regressor = linear_model.HuberRegressor(max_iter=500, epsilon=1.2, alpha=0.00001)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    pred_val = regressor.predict(X_valid)\n",
    "    prediction_list.append(regressor.predict(hstack([test[continue_features].values, test_cat_one_hot])))\n",
    "    best_scores.append(mean_squared_error(y_valid, pred_val))\n",
    "    \n",
    "print(\"5-fold cv mean l2 %.8f\" % np.mean(best_scores))\n",
    "\n",
    "df_submission = pd.read_csv('./data/sample_submit.tsv', sep='\\t', names=['id', 'pred'])\n",
    "\n",
    "df_submission['pred'] = np.exp(np.mean(prediction_list, axis=0))-1\n",
    "df_submission.to_csv('submission_huber.tsv', sep='\\t', header=None, index=False)\n",
    "\n",
    "# 0.01362982"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:02:23.036340Z",
     "start_time": "2019-07-18T16:02:10.597332Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cv mean l2 0.01328986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "splitter = KFold(n_splits=5, shuffle=True, random_state=28)\n",
    "prediction_list = []\n",
    "best_scores = []\n",
    "for train_idx, valid_idx in splitter.split(df_train):\n",
    "    train, valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    test = df_test[df_train.drop(objective, axis=1).columns]\n",
    "    encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "    train_cat_one_hot = encoder.fit_transform(train[categorical_features])\n",
    "    valid_cat_one_hot = encoder.transform(valid[categorical_features])\n",
    "    test_cat_one_hot = encoder.transform(test[categorical_features])\n",
    "    X_train, y_train = hstack([train[continue_features].values, train_cat_one_hot]), np.log(train['keiyaku_pr']+1)\n",
    "    X_valid, y_valid = hstack([valid[continue_features].values, valid_cat_one_hot]), np.log(valid['keiyaku_pr']+1)\n",
    "    regressor = svm.LinearSVR(C=0.1, epsilon=0.01, random_state=28)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    pred_val = regressor.predict(X_valid)\n",
    "    prediction_list.append(regressor.predict(hstack([test[continue_features].values, test_cat_one_hot])))\n",
    "    best_scores.append(mean_squared_error(y_valid, pred_val))\n",
    "    \n",
    "print(\"5-fold cv mean l2 %.8f\" % np.mean(best_scores))\n",
    "\n",
    "df_submission = pd.read_csv('./data/sample_submit.tsv', sep='\\t', names=['id', 'pred'])\n",
    "\n",
    "df_submission['pred'] = np.exp(np.mean(prediction_list, axis=0))-1\n",
    "df_submission.to_csv('submission_linear_svr.tsv', sep='\\t', header=None, index=False)\n",
    "\n",
    "# 0.01328986"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:39:23.920963Z",
     "start_time": "2019-07-17T21:39:23.882751Z"
    }
   },
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('./submission.tsv', sep='\\t', names=['id', 'pred'])     #0.01210079\n",
    "df_2 = pd.read_csv('./submission_ridge.tsv', sep='\\t', names=['id', 'pred'])   #0.01408214\n",
    "df_3 = pd.read_csv('./submission_huber.tsv', sep='\\t', names=['id', 'pred'])  #0.01366592\n",
    "df_4 = pd.read_csv('./submission_linear_svr.tsv', sep='\\t', names=['id', 'pred'])  #0.01330335\n",
    "\n",
    "df_1['pred'] = df_1['pred']*0.75 + df_2['pred']*0.025 + df_3['pred']*0.075 + df_4['pred']*0.15\n",
    "df_1.to_csv('submission_merge.tsv', sep='\\t', index=False, header=None) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
