{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T08:22:09.447952Z",
     "start_time": "2019-07-23T08:22:09.058335Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder, KBinsDiscretizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.sparse import hstack\n",
    "from collections import defaultdict, Counter\n",
    "from ds_tools.ds_tools import CategoricalTransformer\n",
    "import pickle\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T08:22:09.785343Z",
     "start_time": "2019-07-23T08:22:09.449495Z"
    }
   },
   "outputs": [],
   "source": [
    "df_description = pd.read_csv('./data/data_definition.txt', sep='\\t')\n",
    "\n",
    "df_train_genba = pd.read_csv('./data/train_genba.tsv', sep='\\t')\n",
    "df_train_goto = pd.read_csv('./data/train_goto.tsv', sep='\\t')\n",
    "\n",
    "df_train = df_train_goto.merge(df_train_genba, on='pj_no', how='left')\n",
    "\n",
    "df_test_genba = pd.read_csv('./data/test_genba.tsv', sep='\\t')\n",
    "df_test_goto = pd.read_csv('./data/test_goto.tsv', sep='\\t')\n",
    "\n",
    "df_test = df_test_goto.merge(df_test_genba, on='pj_no', how='left')\n",
    "test_surface = df_test['tc_mseki']\n",
    "\n",
    "df_train.drop(['id', 'kaoku_um', 'shu_sogi'], axis=1, inplace=True)\n",
    "df_test.drop(['id', 'kaoku_um', 'shu_sogi'], axis=1, inplace=True)\n",
    "\n",
    "continue_features = list(df_description[(df_description['データ型'] == '数値') & (df_description['項目名'] != 'pj_no')]['項目名'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T08:22:12.755097Z",
     "start_time": "2019-07-23T08:22:09.787007Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine(row, combine_list, tup):\n",
    "    l = set()\n",
    "    for col in tup:\n",
    "        if pd.notnull(row[col]):\n",
    "            l.add(row[col])\n",
    "    combine_list.append(','.join(l))\n",
    "\n",
    "\n",
    "combine_cols = [('yoto', 100), ('road_hk', 100), ('road_sb', 1), ('toshikuiki', 10), ('hokakisei', 10), ('kobetsu', 10)]\n",
    "for i, tup in enumerate([['yoto1', 'yoto2'], ['road1_hk', 'road2_hk', 'road3_hk', 'road4_hk'], \n",
    "                         ['road1_sb', 'road2_sb', 'road3_sb', 'road4_sb'], ['toshikuiki1', 'toshikuiki2'], \n",
    "                        ['hokakisei1', 'hokakisei2', 'hokakisei3', 'hokakisei4'],\n",
    "                         ['kobetsu1', 'kobetsu2', 'kobetsu3', 'kobetsu4']]):\n",
    "    combine_train = []\n",
    "    combine_test = []\n",
    "    \n",
    "    combine_col_name = combine_cols[i][0]\n",
    "    _ = df_train.apply(lambda row: combine(row, combine_train, tup), axis=1)\n",
    "    _ = df_test.apply(lambda row: combine(row, combine_test, tup), axis=1)\n",
    "\n",
    "    count_vectorizer = CountVectorizer(min_df=combine_cols[i][1])\n",
    "    combine_train_matrix = count_vectorizer.fit_transform(combine_train).todense()\n",
    "    combine_test_matrix = count_vectorizer.transform(combine_test).todense()\n",
    "    for i in range(combine_train_matrix.shape[1]):\n",
    "        df_train['%s_%d' % (combine_col_name, i)] = combine_train_matrix[:, i]\n",
    "        df_test['%s_%d' % (combine_col_name, i)] = combine_test_matrix[:, i]\n",
    "    for col in tup:\n",
    "        if col not in ['toshikuiki1']:\n",
    "            df_train.drop(col, axis=1, inplace=True)\n",
    "            df_test.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T08:22:13.635701Z",
     "start_time": "2019-07-23T08:22:12.756825Z"
    }
   },
   "outputs": [],
   "source": [
    "coordinates = pickle.load(open('./data/coordinates.bin', 'rb'))\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    df['lat'] = df['jukyo'].apply(lambda j: coordinates[j]['results'][0]['geometry']['location']['lat'] if coordinates[j]['results'] else np.nan)\n",
    "    df['lng'] = df['jukyo'].apply(lambda j: coordinates[j]['results'][0]['geometry']['location']['lng'] if coordinates[j]['results'] else np.nan)\n",
    "    \n",
    "def fill_city_name(name):\n",
    "    if '市' not in name and '郡' not in name:\n",
    "        name = '市' + name\n",
    "    return name\n",
    "\n",
    "def split_address(df):\n",
    "    df['jukyo'] = df['jukyo'].str.replace(r'[ヶｹ]', 'ケ')\n",
    "    df['jukyo'] = df['jukyo'].apply(fill_city_name)\n",
    "    city_split = df['jukyo'].str.split(r'[市郡]', n=1, expand=True)\n",
    "    df['city'] = city_split[0]\n",
    "    street_split = city_split[1].str.split(r'[町区]', n=1, expand=True)\n",
    "    df['street'] = street_split[0]\n",
    "    df['address_detail'] = street_split[1].str.strip().str.replace('大字', '').replace('', None)\n",
    "    return df\n",
    "\n",
    "df_train = split_address(df_train)\n",
    "df_test = split_address(df_test)\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    df['station_name_prefix'] = df['rosen_nm1'].str.slice(stop=2)\n",
    "    df['city_toshikuiki1'] = df['city'] + ' ' + df['toshikuiki1']\n",
    "    df.drop('toshikuiki1', axis=1, inplace=True)\n",
    "    \n",
    "    for col in ['mseki_rd_hb', 'road3_fi', 'rosenka_hb', 'kempei2', 'road2_mg', 'kaoku_hb', 'bus_hon']:\n",
    "        df[col].replace(0.0, np.nan, inplace=True)\n",
    "    \n",
    "for col in ['chiseki_kb_hb', 'magutchi']:\n",
    "    col_mean = pd.concat([df_train[col], df_test[col]]).mean()\n",
    "    df_train[col] = df_train[col].fillna(col_mean)\n",
    "    df_test[col] = df_test[col].fillna(col_mean)\n",
    "\n",
    "for col in ['chiseki_js_hb', 'tc_mseki_min_hb', 'chiseki_kb_hb', 'tc_mseki', 'magutchi']:\n",
    "    binning = KBinsDiscretizer(n_bins=30, encode='ordinal', strategy='uniform')\n",
    "    df_train[col+'_bins'] = binning.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "    df_test[col+'_bins'] = binning.transform(df_test[col].values.reshape(-1, 1))\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    df['tc_mseki_bins_road_st'] = df['tc_mseki_bins'].astype(str) + ' ' + df['road_st']\n",
    "    \n",
    "def make_percentage_features(numeric_feature, categorical_feature):\n",
    "    global continue_features\n",
    "    df_combined = pd.concat([df_train, df_test], sort=True).reset_index(drop=True)\n",
    "    mean_label = '%s_to_mean_%s' % (numeric_feature, categorical_feature)\n",
    "    df_combined[mean_label] = df_combined[numeric_feature] / df_combined.groupby([categorical_feature])[numeric_feature].transform('mean')\n",
    "    df_train[mean_label] = df_combined[mean_label].iloc[:len(df_train)]\n",
    "    df_test[mean_label] = df_combined[mean_label].iloc[len(df_train):].reset_index(drop=True)\n",
    "    continue_features += [mean_label]\n",
    "    \n",
    "make_percentage_features('magutchi', 'eki_nm1')\n",
    "make_percentage_features('tt_mseki', 'eki_nm1')\n",
    "make_percentage_features('tc_mseki', 'eki_nm1')\n",
    "make_percentage_features('niwasaki', 'eki_nm1')\n",
    "make_percentage_features('rosenka_hb', 'eki_nm1')\n",
    "make_percentage_features('magutchi', 'city')\n",
    "make_percentage_features('tt_mseki', 'city')\n",
    "make_percentage_features('tc_mseki', 'city')\n",
    "make_percentage_features('niwasaki', 'city')\n",
    "make_percentage_features('rosenka_hb', 'city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T08:22:14.015752Z",
     "start_time": "2019-07-23T08:22:13.637162Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splitter = KFold(n_splits=5, shuffle=True, random_state=28)\n",
    "price_stats = []\n",
    "for train_idx, valid_idx in splitter.split(df_train):\n",
    "    price_stats_by_city = defaultdict(dict)\n",
    "    for city, group in df_train.iloc[train_idx].groupby('city'):\n",
    "        price_list = group['keiyaku_pr']/group['tc_mseki']\n",
    "        price_stats_by_city[city]['price_by_city_mean'] = price_list.mean()\n",
    "        price_stats_by_city[city]['price_by_city_median'] = price_list.median()\n",
    "        price_stats_by_city[city]['price_by_city_min'] = price_list.min()\n",
    "        price_stats_by_city[city]['price_by_city_max'] = price_list.max()\n",
    "        price_stats_by_city[city]['price_by_city_std'] = price_list.std()\n",
    "        price_stats_by_city[city]['price_by_city_count'] = len(price_list)\n",
    "    for i, city in enumerate(df_train.iloc[valid_idx]['city']):\n",
    "        price_stats.append((valid_idx[i], price_stats_by_city[city]))\n",
    "\n",
    "price_stats_test = [] \n",
    "price_stats_by_city = defaultdict(dict)\n",
    "for city, group in df_train.groupby('city'):\n",
    "    price_list = group['keiyaku_pr']/group['tc_mseki']\n",
    "    price_stats_by_city[city]['price_by_city_mean'] = price_list.mean()\n",
    "    price_stats_by_city[city]['price_by_city_median'] = price_list.median()\n",
    "    price_stats_by_city[city]['price_by_city_min'] = price_list.min()\n",
    "    price_stats_by_city[city]['price_by_city_max'] = price_list.max()\n",
    "    price_stats_by_city[city]['price_by_city_std'] = price_list.std()\n",
    "    price_stats_by_city[city]['price_by_city_count'] = len(price_list)\n",
    "for city in df_test['city']:\n",
    "    price_stats_test.append(price_stats_by_city[city])\n",
    "    \n",
    "df_price_stats = pd.DataFrame([x[1] for x in sorted(price_stats, key=lambda x: x[0])])\n",
    "df_price_stats_test = pd.DataFrame(price_stats_test)\n",
    "\n",
    "df_train = pd.concat([df_train, df_price_stats], axis=1)\n",
    "df_test = pd.concat([df_test, df_price_stats_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T08:22:15.201523Z",
     "start_time": "2019-07-23T08:22:14.017164Z"
    }
   },
   "outputs": [],
   "source": [
    "continue_features += ['price_by_city_mean', 'price_by_city_median', 'price_by_city_min', 'price_by_city_max', \n",
    "                      'price_by_city_std', 'price_by_city_count', 'lng', 'lat']\n",
    "objective = 'keiyaku_pr'\n",
    "categorical_features = list(df_train)\n",
    "\n",
    "for f in continue_features+[objective]:\n",
    "    if f in categorical_features:\n",
    "        categorical_features.remove(f)\n",
    "        \n",
    "for col in categorical_features:\n",
    "    if col not in ['pj_no']:\n",
    "        ct = CategoricalTransformer(min_freq=3)\n",
    "        df_train[col] = ct.fit_transform(df_train[col])\n",
    "        df_test[col] = ct.transform(df_test[col])\n",
    "        \n",
    "for col in continue_features:\n",
    "    if col != 'keiyaku_pr':\n",
    "        scaler = MinMaxScaler()\n",
    "        df_train[col] = scaler.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "        df_test[col] = scaler.transform(df_test[col].values.reshape(-1, 1))\n",
    "        df_train[col] = df_train[col].fillna(df_train[col].mean())\n",
    "        df_test[col] = df_test[col].fillna(df_test[col].mean())\n",
    "    \n",
    "df_test['keiyaku_pr'] = 0\n",
    "continue_features.remove('keiyaku_pr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T08:22:25.005623Z",
     "start_time": "2019-07-23T08:22:15.202938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cv mean l2 0.01266222\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "splitter = KFold(n_splits=5, shuffle=True, random_state=28)\n",
    "prediction_list = []\n",
    "best_scores = []\n",
    "df_train.fillna(0, inplace=True)\n",
    "df_test.fillna(0, inplace=True)\n",
    "for train_idx, valid_idx in splitter.split(df_train):\n",
    "    train, valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    test = df_test[df_train.drop(objective, axis=1).columns]\n",
    "    encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "    train_cat_one_hot = encoder.fit_transform(train[categorical_features])\n",
    "    valid_cat_one_hot = encoder.transform(valid[categorical_features])\n",
    "    test_cat_one_hot = encoder.transform(test[categorical_features])\n",
    "    X_train, y_train = hstack([train[continue_features].values, train_cat_one_hot]), np.log(train['keiyaku_pr']+1)\n",
    "    X_valid, y_valid = hstack([valid[continue_features].values, valid_cat_one_hot]), np.log(valid['keiyaku_pr']+1)\n",
    "    regressor = linear_model.Ridge(alpha=3, tol=0.0001, random_state=28)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    pred_val = regressor.predict(X_valid)\n",
    "    prediction_list.append(regressor.predict(hstack([test[continue_features].values, test_cat_one_hot])))\n",
    "    best_scores.append(mean_squared_error(y_valid, pred_val))\n",
    "    \n",
    "print(\"5-fold cv mean l2 %.8f\" % np.mean(best_scores))\n",
    "\n",
    "df_submission = pd.read_csv('./data/sample_submit.tsv', sep='\\t', names=['id', 'pred'])\n",
    "\n",
    "df_submission['pred'] = np.exp(np.mean(prediction_list, axis=0))-1\n",
    "df_submission.to_csv('submission_ridge.tsv', sep='\\t', header=None, index=False)\n",
    "\n",
    "# 0.01266222"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuberRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T08:22:58.806558Z",
     "start_time": "2019-07-23T08:22:25.007707Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cv mean l2 0.01252956\n"
     ]
    }
   ],
   "source": [
    "splitter = KFold(n_splits=5, shuffle=True, random_state=28)\n",
    "prediction_list = []\n",
    "best_scores = []\n",
    "for train_idx, valid_idx in splitter.split(df_train):\n",
    "    train, valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    test = df_test[df_train.drop(objective, axis=1).columns]\n",
    "    encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "    train_cat_one_hot = encoder.fit_transform(train[categorical_features])\n",
    "    valid_cat_one_hot = encoder.transform(valid[categorical_features])\n",
    "    test_cat_one_hot = encoder.transform(test[categorical_features])\n",
    "    X_train, y_train = hstack([train[continue_features].values, train_cat_one_hot]), np.log(train['keiyaku_pr']+1)\n",
    "    X_valid, y_valid = hstack([valid[continue_features].values, valid_cat_one_hot]), np.log(valid['keiyaku_pr']+1)\n",
    "    regressor = linear_model.HuberRegressor(max_iter=500, epsilon=1.2, alpha=0.00001)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    pred_val = regressor.predict(X_valid)\n",
    "    prediction_list.append(regressor.predict(hstack([test[continue_features].values, test_cat_one_hot])))\n",
    "    best_scores.append(mean_squared_error(y_valid, pred_val))\n",
    "    \n",
    "print(\"5-fold cv mean l2 %.8f\" % np.mean(best_scores))\n",
    "\n",
    "df_submission = pd.read_csv('./data/sample_submit.tsv', sep='\\t', names=['id', 'pred'])\n",
    "\n",
    "df_submission['pred'] = np.exp(np.mean(prediction_list, axis=0))-1\n",
    "df_submission.to_csv('submission_huber.tsv', sep='\\t', header=None, index=False)\n",
    "\n",
    "# 0.01252956"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T08:23:16.772331Z",
     "start_time": "2019-07-23T08:22:58.808151Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cv mean l2 0.01226565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "splitter = KFold(n_splits=5, shuffle=True, random_state=28)\n",
    "prediction_list = []\n",
    "best_scores = []\n",
    "for train_idx, valid_idx in splitter.split(df_train):\n",
    "    train, valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    test = df_test[df_train.drop(objective, axis=1).columns]\n",
    "    encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "    train_cat_one_hot = encoder.fit_transform(train[categorical_features])\n",
    "    valid_cat_one_hot = encoder.transform(valid[categorical_features])\n",
    "    test_cat_one_hot = encoder.transform(test[categorical_features])\n",
    "    X_train, y_train = hstack([train[continue_features].values, train_cat_one_hot]), np.log(train['keiyaku_pr']+1)\n",
    "    X_valid, y_valid = hstack([valid[continue_features].values, valid_cat_one_hot]), np.log(valid['keiyaku_pr']+1)\n",
    "    regressor = svm.LinearSVR(C=0.1, epsilon=0.01, intercept_scaling=1.5, random_state=28)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    pred_val = regressor.predict(X_valid)\n",
    "    prediction_list.append(regressor.predict(hstack([test[continue_features].values, test_cat_one_hot])))\n",
    "    best_scores.append(mean_squared_error(y_valid, pred_val))\n",
    "    \n",
    "print(\"5-fold cv mean l2 %.8f\" % np.mean(best_scores))\n",
    "\n",
    "df_submission = pd.read_csv('./data/sample_submit.tsv', sep='\\t', names=['id', 'pred'])\n",
    "\n",
    "df_submission['pred'] = np.exp(np.mean(prediction_list, axis=0))-1\n",
    "df_submission.to_csv('submission_linear_svr.tsv', sep='\\t', header=None, index=False)\n",
    "\n",
    "# 0.01226565"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T08:23:16.806312Z",
     "start_time": "2019-07-23T08:23:16.773695Z"
    }
   },
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('./submission.tsv', sep='\\t', names=['id', 'pred'])\n",
    "df_2 = pd.read_csv('./submission_ridge.tsv', sep='\\t', names=['id', 'pred'])\n",
    "df_3 = pd.read_csv('./submission_huber.tsv', sep='\\t', names=['id', 'pred'])\n",
    "df_4 = pd.read_csv('./submission_linear_svr.tsv', sep='\\t', names=['id', 'pred'])\n",
    "\n",
    "df_1['pred'] = df_1['pred']*0.6 + ((df_2['pred']*0.25 + df_3['pred']*0.75)*0.4 + df_4['pred']*0.6)*0.4\n",
    "df_1.to_csv('submission_merge.tsv', sep='\\t', index=False, header=None) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
