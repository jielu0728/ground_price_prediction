{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T12:48:28.161490Z",
     "start_time": "2019-07-19T12:48:27.650388Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder, KBinsDiscretizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.sparse import hstack\n",
    "from collections import defaultdict, Counter\n",
    "from ds_tools.ds_tools import CategoricalTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T12:48:28.500084Z",
     "start_time": "2019-07-19T12:48:28.163070Z"
    }
   },
   "outputs": [],
   "source": [
    "df_description = pd.read_csv('./data/data_definition.txt', sep='\\t')\n",
    "\n",
    "df_train_genba = pd.read_csv('./data/train_genba.tsv', sep='\\t')\n",
    "df_train_goto = pd.read_csv('./data/train_goto.tsv', sep='\\t')\n",
    "\n",
    "df_train = df_train_goto.merge(df_train_genba, on='pj_no', how='left')\n",
    "df_train.drop('id', axis=1, inplace=True)\n",
    "\n",
    "df_test_genba = pd.read_csv('./data/test_genba.tsv', sep='\\t')\n",
    "df_test_goto = pd.read_csv('./data/test_goto.tsv', sep='\\t')\n",
    "\n",
    "df_test = df_test_goto.merge(df_test_genba, on='pj_no', how='left')\n",
    "test_surface = df_test['tc_mseki']\n",
    "df_test.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T12:48:28.595433Z",
     "start_time": "2019-07-19T12:48:28.501789Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_city_name(name):\n",
    "    if '市' not in name and '郡' not in name:\n",
    "        name = '市' + name\n",
    "    return name\n",
    "\n",
    "def split_address(df):\n",
    "    df['jukyo'] = df['jukyo'].str.slice(start=3).str.replace(r'[ヶｹ]', 'ケ')\n",
    "    df['jukyo'] = df['jukyo'].apply(fill_city_name)\n",
    "    city_split = df['jukyo'].str.split(r'[市郡]', n=1, expand=True)\n",
    "    df['city'] = city_split[0]\n",
    "    street_split = city_split[1].str.split(r'[町区]', n=1, expand=True)\n",
    "    df['street'] = street_split[0]\n",
    "    df['address_detail'] = street_split[1].str.strip().replace('', None)\n",
    "    return df\n",
    "\n",
    "df_train = split_address(df_train)\n",
    "df_test = split_address(df_test)\n",
    "\n",
    "df_train.drop(['kaoku_um', 'toshikuiki2', 'shu_sogi'], axis=1, inplace=True)\n",
    "df_test.drop(['kaoku_um', 'toshikuiki2', 'shu_sogi'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T12:48:29.570195Z",
     "start_time": "2019-07-19T12:48:28.597215Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine(row, combine_list, tup):\n",
    "    l = set()\n",
    "    for col in tup:\n",
    "        if pd.notnull(row[col]):\n",
    "            l.add(row[col])\n",
    "    combine_list.append(','.join(l))\n",
    "\n",
    "\n",
    "combine_cols = [('yoto', 100), ('road_hk', 100)]\n",
    "for i, tup in enumerate([['yoto1', 'yoto2'], ['road1_hk', 'road2_hk', 'road3_hk', 'road4_hk']]):\n",
    "    combine_train = []\n",
    "    combine_test = []\n",
    "    \n",
    "    combine_col_name = combine_cols[i][0]\n",
    "    _ = df_train.apply(lambda row: combine(row, combine_train, tup), axis=1)\n",
    "    _ = df_test.apply(lambda row: combine(row, combine_test, tup), axis=1)\n",
    "\n",
    "    count_vectorizer = CountVectorizer(min_df=combine_cols[i][1])\n",
    "    combine_train_matrix = count_vectorizer.fit_transform(combine_train).todense()\n",
    "    combine_test_matrix = count_vectorizer.transform(combine_test).todense()\n",
    "    for i in range(combine_train_matrix.shape[1]):\n",
    "        df_train['%s_%d' % (combine_col_name, i)] = combine_train_matrix[:, i]\n",
    "        df_test['%s_%d' % (combine_col_name, i)] = combine_test_matrix[:, i]\n",
    "    for col in tup:\n",
    "        df_train.drop(col, axis=1, inplace=True)\n",
    "        df_test.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T12:48:29.961431Z",
     "start_time": "2019-07-19T12:48:29.571660Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splitter = KFold(n_splits=5, shuffle=True, random_state=28)\n",
    "price_stats = []\n",
    "for train_idx, valid_idx in splitter.split(df_train):\n",
    "    price_stats_by_city = defaultdict(dict)\n",
    "    for city, group in df_train.iloc[train_idx].groupby('city'):\n",
    "        price_list = group['keiyaku_pr']/group['tc_mseki']\n",
    "        price_stats_by_city[city]['price_by_city_mean'] = price_list.mean()\n",
    "        price_stats_by_city[city]['price_by_city_median'] = price_list.median()\n",
    "        price_stats_by_city[city]['price_by_city_min'] = price_list.min()\n",
    "        price_stats_by_city[city]['price_by_city_max'] = price_list.max()\n",
    "        price_stats_by_city[city]['price_by_city_std'] = price_list.std()\n",
    "        price_stats_by_city[city]['price_by_city_count'] = len(price_list)\n",
    "    for i, city in enumerate(df_train.iloc[valid_idx]['city']):\n",
    "        price_stats.append((valid_idx[i], price_stats_by_city[city]))\n",
    "\n",
    "price_stats_test = [] \n",
    "price_stats_by_city = defaultdict(dict)\n",
    "for city, group in df_train.groupby('city'):\n",
    "    price_list = group['keiyaku_pr']/group['tc_mseki']\n",
    "    price_stats_by_city[city]['price_by_city_mean'] = price_list.mean()\n",
    "    price_stats_by_city[city]['price_by_city_median'] = price_list.median()\n",
    "    price_stats_by_city[city]['price_by_city_min'] = price_list.min()\n",
    "    price_stats_by_city[city]['price_by_city_max'] = price_list.max()\n",
    "    price_stats_by_city[city]['price_by_city_std'] = price_list.std()\n",
    "    price_stats_by_city[city]['price_by_city_count'] = len(price_list)\n",
    "for city in df_test['city']:\n",
    "    price_stats_test.append(price_stats_by_city[city])\n",
    "    \n",
    "df_price_stats = pd.DataFrame([x[1] for x in sorted(price_stats, key=lambda x: x[0])])\n",
    "df_price_stats_test = pd.DataFrame(price_stats_test)\n",
    "\n",
    "df_train = pd.concat([df_train, df_price_stats], axis=1)\n",
    "df_test = pd.concat([df_test, df_price_stats_test], axis=1)\n",
    "\n",
    "\n",
    "for col in ['mseki_rd_hb', 'road3_fi', 'rosenka_hb', 'kempei2', 'road2_mg', 'kaoku_hb', 'bus_hon']:\n",
    "    df_train[col].replace(0.0, np.nan, inplace=True)\n",
    "    df_test[col].replace(0.0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T12:48:29.988566Z",
     "start_time": "2019-07-19T12:48:29.962860Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['station_name_prefix'] = df_train['rosen_nm1'].str.slice(stop=2)\n",
    "df_test['station_name_prefix'] = df_test['rosen_nm1'].str.slice(stop=2)\n",
    "\n",
    "to_decompose = []\n",
    "for col in df_train:\n",
    "    if col.startswith('sho_') or col.startswith('shu_'):\n",
    "        df_train[col] = df_train[col].fillna('No')\n",
    "        df_test[col] = df_test[col].fillna('No')\n",
    "        to_decompose.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T12:48:30.966431Z",
     "start_time": "2019-07-19T12:48:29.990062Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-eec1a0bd6b85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mrepresentation_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecomposer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_decompose\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mrepresentation_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecomposer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_decompose\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrepresentation_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'repre_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepresentation_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'repre_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepresentation_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "continue_features = list(df_description[(df_description['データ型'] == '数値') & (df_description['項目名'] != 'pj_no')]['項目名'])\n",
    "continue_features += ['price_by_city_mean', 'price_by_city_median', 'price_by_city_min', 'price_by_city_max', \n",
    "                      'price_by_city_std', 'price_by_city_count']\n",
    "objective = 'keiyaku_pr'\n",
    "categorical_features = list(df_train)\n",
    "\n",
    "for f in continue_features+[objective]:\n",
    "    if f in categorical_features:\n",
    "        categorical_features.remove(f)\n",
    "        \n",
    "for col in categorical_features:\n",
    "    if col not in ['pj_no']:\n",
    "        ct = CategoricalTransformer(min_freq=3)\n",
    "        df_train[col] = ct.fit_transform(df_train[col])\n",
    "        df_test[col] = ct.transform(df_test[col])\n",
    "        \n",
    "for col in continue_features:\n",
    "    if col != 'keiyaku_pr':\n",
    "        scaler = MinMaxScaler()\n",
    "        df_train[col] = scaler.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "        df_test[col] = scaler.transform(df_test[col].values.reshape(-1, 1))\n",
    "    \n",
    "df_test['keiyaku_pr'] = 0\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "decomposer = NMF(n_components=5, random_state=28)\n",
    "representation_train = decomposer.fit_transform(df_train[to_decompose])\n",
    "representation_test = decomposer.transform(df_test[to_decompose])\n",
    "for i in range(representation_train.shape[1]:\n",
    "    df_train['repre_%d' % i] = representation_train[:, i]\n",
    "    df_test['repre_%d' % i] = representation_test[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T12:48:30.968186Z",
     "start_time": "2019-07-19T12:48:07.508Z"
    }
   },
   "outputs": [],
   "source": [
    "splitter = KFold(n_splits=5, shuffle=True, random_state=28)\n",
    "prediction_list = []\n",
    "best_scores = []\n",
    "for train_idx, valid_idx in splitter.split(df_train):\n",
    "    train, valid = df_train.iloc[train_idx], df_train.iloc[valid_idx]\n",
    "    X_train, y_train = train.drop('keiyaku_pr', axis=1), np.log(train['keiyaku_pr']+1)\n",
    "    X_valid, y_valid = valid.drop('keiyaku_pr', axis=1), np.log(valid['keiyaku_pr']+1)\n",
    "    regressor = lgb.LGBMRegressor(n_estimators=20000, learning_rate=0.01, silent=False, random_state=28,\n",
    "                                  bagging_fraction=0.8, bagging_freq=1, feature_fraction=0.8, lambda_l1=0.05, \n",
    "                                  max_depth=6, objective='huber')\n",
    "    regressor.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=500,\n",
    "                  categorical_feature=categorical_features)\n",
    "    prediction_list.append(regressor.predict(df_test[df_train.drop(objective, axis=1).columns]))\n",
    "    best_scores.append(regressor.best_score_['valid_0']['huber'])\n",
    "\n",
    "print(\"5-fold cv mean l2 %.8f\" % np.mean(best_scores))\n",
    "\n",
    "df_submission = pd.read_csv('./data/sample_submit.tsv', sep='\\t', names=['id', 'pred'])\n",
    "\n",
    "df_submission['pred'] = np.exp(np.mean(prediction_list, axis=0))-1\n",
    "df_submission.to_csv('submission.tsv', sep='\\t', header=None, index=False)\n",
    "\n",
    "# 0.00536728"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
